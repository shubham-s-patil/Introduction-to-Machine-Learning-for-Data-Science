{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 3: Azure Cognitive Services\n\nJust as you created a web service that could consume data and return predictions, so there are many AI software-as-a-service (SaaS) offerings on the web that will return predictions or classifications based on data you supply to them. One family of these is Microsoft Azure Cognitive Services.\n\nThe advantage of using cloud-based services is that they provide cutting-edge models that you can access without having to train them. This can help accelerate both your exploration and use of ML.\n\nAzure provides Cognitive Services APIs that can be consumed using Python to conduct image recognition, speech recognition, and text recognition, just to name a few. For the purposes of this notebook, we're going to look at using the Computer Vision API and the Text Analytics API.\n\nFirst, we’ll start by obtaining a Cognitive Services API key. Note that you can get a free key for seven days, and then you'll be required to pay.\n\nTo learn more about pricing for Cognitive Services, see https://azure.microsoft.com/en-us/pricing/details/cognitive-services/\n\nBrowse to **Try Azure Cognitive Services** at https://azure.microsoft.com/en-us/try/cognitive-services/\n\n1. Select **Vision API**.\n2. Select **Computer Vision**.\n3. Click **Get API key**.\n4. If prompted for credentials, select **Free 7-day trial**.\n\nComplete the above steps to also retrieve a Text Analytics API key from the Language APIs category. (You can also do this by scrolling down on the page with your API keys and clicking **Add** under the appropriate service.)\n\nOnce you have your API keys in hand, you're ready to start.\n\n> **Learning goal:** By the end of this part, you should have a basic comfort with accessing cloud-based cognitive services by API from a Python environment."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure Cognitive Services Computer Vision\n\nComputer vision is a hot topic in academic AI research and in business, medical, government, and environmental applications. We will explore it here by seeing firsthand how computers can tag and identify images.\n\nThe first step in using the Cognitive Services Computer Vision API is to create a client object using the ComputerVisionClient class.\n\nReplace **ACCOUNT_ENDPOINT** with the account endpoint provided from the free trial. Replace **ACCOUNT_KEY** with the account key provided from the free trial."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "!pip install azure-cognitiveservices-vision-computervision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\nfrom msrest.authentication import CognitiveServicesCredentials\n\n# Get endpoint and key from environment variables\nendpoint = 'ACCOUNT_ENDPOINT'\n# Example: endpoint = 'https://westcentralus.api.cognitive.microsoft.com'\nkey = 'ACCOUNT_KEY'\n# Example key = '1234567890abcdefghijklmnopqrstuv\n\n# Set credentials\ncredentials = CognitiveServicesCredentials(key)\n\n# Create client\nclient = ComputerVisionClient(endpoint, credentials)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have a client object to work with, let's see what we can do.\n\nUsing analyze_image, we can see the properties of the image with VisualFeatureTypes.tags."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "url = 'https://cdn.pixabay.com/photo/2014/05/02/23/54/times-square-336508_960_720.jpg'\n\nimage_analysis = client.analyze_image(url,visual_features=[VisualFeatureTypes.tags])\n\nfor tag in image_analysis.tags:\n    print(tag)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How can you use the code above to also see the description using VisualFeatureTypes property?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's look at the subject domain of the image. An example of a domain is celebrity.\nAs of now, the analyze_image_by_domain method only supports celebrities and landmarks domain-specific models."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# This will list the available subject domains \nmodels = client.list_models()\n\nfor x in models.models_property:\n    print(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's analyze an image by domain:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Type of prediction\ndomain = \"landmarks\"\n\n# Public-domain image of Seattle\nurl = \"https://images.pexels.com/photos/37350/space-needle-seattle-washington-cityscape.jpg\"\n\n# English-language response\nlanguage = \"en\"\n\nanalysis = client.analyze_image_by_domain(domain, url, language)\n\nfor landmark in analysis.result[\"landmarks\"]:\n    print(landmark[\"name\"])\n    print(landmark[\"confidence\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How can you use the code above to predict an image of a celebrity?\n# Using this image, https://images.pexels.com/photos/270968/pexels-photo-270968.jpeg?\n# Remember that the domains were printed out earlier.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see how we can get a text description of an image using the describe_image method. Use max_descriptions to retrieve how many descriptions of the image the API service can find. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "domain = \"landmarks\"\nurl = \"https://images.pexels.com/photos/726484/pexels-photo-726484.jpeg\"\nlanguage = \"en\"\nmax_descriptions = 3\n\nanalysis = client.describe_image(url, max_descriptions, language)\n\nfor caption in analysis.captions:\n    print(caption.text)\n    print(caption.confidence)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What other descriptions can be found with other images?\n# What happens if you change the count of descriptions to output?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's say that the images contain text. How do we retrieve that information? There are two methods that need to be used for this type of call. Batch_read_file and get_read_operation_result. TextOperationStatusCodes is used to ensure that the batch_read_file call is completed before the text is read from the image. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# import models\nfrom azure.cognitiveservices.vision.computervision.models import TextRecognitionMode\nfrom azure.cognitiveservices.vision.computervision.models import TextOperationStatusCodes\nimport time\n\nurl = \"https://images.pexels.com/photos/6375/quote-chalk-think-words.jpg\"\nmode = TextRecognitionMode.handwritten\nraw = True\ncustom_headers = None\nnumberOfCharsInOperationId = 36\n\n# Async SDK call\nrawHttpResponse = client.batch_read_file(url, mode, custom_headers,  raw)\n\n# Get ID from returned headers\noperationLocation = rawHttpResponse.headers[\"Operation-Location\"]\nidLocation = len(operationLocation) - numberOfCharsInOperationId\noperationId = operationLocation[idLocation:]\n\n# SDK call\nwhile True:\n    result = client.get_read_operation_result(operationId)\n    if result.status not in ['NotStarted', 'Running']:\n        break\n    time.sleep(1)\n\n# Get data\nif result.status == TextOperationStatusCodes.succeeded:\n    for textResult in result.recognition_results:\n        for line in textResult.lines:\n            print(line.text)\n            print(line.bounding_box)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What other images with words can be analyzed?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can find addition Cognitive Services demonstrations at the following URLs:\n - https://aidemos.microsoft.com/\n - https://github.com/microsoft/computerscience/blob/master/Events%20and%20Hacks/Student%20Hacks/hackmit/cogservices_demos/\n - https://azure.microsoft.com/en-us/services/cognitive-services/directory/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Images come in varying sizes, and there might be cases where you want to create a thumbnail of the image. For this, we need to install the Pillow library, which you can learn about at https://python-pillow.org/. Pillow is the PIL fork, or Python Imaging Library, which allows for image processing. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Install Pillow\n!pip install Pillow",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that the Pillow library is installed, we will import the Image module and create a thumbnail from a provided image. (Once generated, you can find the thumbnail image in your project folder on Azure Notebooks.)"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Pillow package\nfrom PIL import Image\n\n# IO package to create local image\nimport io\n\nwidth = 50\nheight = 50\nurl = \"https://images.pexels.com/photos/37350/space-needle-seattle-washington-cityscape.jpg\"\n\nthumbnail = client.generate_thumbnail(width, height, url)\n\nfor x in thumbnail:\n    image = Image.open(io.BytesIO(x))\n\nimage.save('thumbnail.jpg')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** In this subsection, you explored how to access computer-vision cognitive services by API. Specifically, you used tools to analyze and describe images that you submitted to these services."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure Cognitive Services Text Analytics\n\nAnother area where cloud-based AI shines is text analytics. Like computer vision, identifying and pulling meaning from natural human languages is really the intersection of a lot of specialized disciplines, so using cloud services for it provides an economical means of tapping a lot of cognitive horsepower.\n\nTo prepare to use the Cognitive Services Text Analytics API, the requests library must be imported, along with the ability to print out JSON formats."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import requests\n# pprint is pretty print (formats the JSON)\nfrom pprint import pprint\nfrom IPython.display import HTML",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Replace 'ACCOUNT_KEY' with the API key that was created during the creation of the seven-day free trial account."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "subscription_key = 'ACCOUNT_KEY'\nassert subscription_key\n\n# If using a Free Trial account, this URL does not need to be udpated.\n# If using a paid account, verify that it matches the region where the \n# Text Analytics Service was setup.\ntext_analytics_base_url = \"https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Text Analytics API"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now it's time to start processing some text languages.\n\nTo verify the URL endpoint for text_analytics_base_url, run the following:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "language_api_url = text_analytics_base_url + \"languages\"\nprint(language_api_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The API requires that the payload be formatted in the form of documents containing `id` and `text` attributes:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "documents = { 'documents': [\n    { 'id': '1', 'text': 'This is a document written in English.' },\n    { 'id': '2', 'text': 'Este es un documento escrito en Español.' },\n    { 'id': '3', 'text': '这是一个用中文写的文件' },\n    { 'id': '4', 'text': 'Ez egy magyar nyelvű dokumentum.' },\n    { 'id': '5', 'text': 'Dette er et dokument skrevet på dansk.' },\n    { 'id': '6', 'text': 'これは日本語で書かれた文書です。' }\n]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next lines of code call the API service using the requests library to determine the languages that were passed in from the documents:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\nresponse  = requests.post(language_api_url, headers=headers, json=documents)\nlanguages = response.json()\npprint(languages)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next line of code outputs the documents in a table format with the language information for each document:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "table = []\nfor document in languages[\"documents\"]:\n    text  = next(filter(lambda d: d[\"id\"] == document[\"id\"], documents[\"documents\"]))[\"text\"]\n    langs = \", \".join([\"{0}({1})\".format(lang[\"name\"], lang[\"score\"]) for lang in document[\"detectedLanguages\"]])\n    table.append(\"<tr><td>{0}</td><td>{1}</td>\".format(text, langs))\nHTML(\"<table><tr><th>Text</th><th>Detected languages(scores)</th></tr>{0}</table>\".format(\"\\n\".join(table)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service did a pretty good job of identifying the languages. It did confidently identify the Danish phrase as being Norwegian, but in fairness, even linguists argue as to whether Danish and Norwegian constitute distinct languages or are dialects of the same language. (**Note:** Danes and Norwegians have no doubts on the subject.)\n\n### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create another document set of text and use the text analytics API to detect the language for the text. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Sentiment Analysis API"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we know how to use the Text Analytics API to detect the language, let's use it for sentiment analysis. Basically, the computers at the other end of the API connection will judge the sentiments of written phrases (anywhere on the spectrum of positive to negative) based solely on the context clues provided by the text."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Verify the API URl source for the Sentiment Analysis API\nsentiment_api_url = text_analytics_base_url + \"sentiment\"\nprint(sentiment_api_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As above, the Sentiment Analysis API requires the language to be passed in as documents with `id` and `text` attributes."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "documents = {'documents' : [\n  {'id': '1', 'language': 'en', 'text': 'I had a wonderful experience! The rooms were wonderful and the staff was helpful.'},\n  {'id': '2', 'language': 'en', 'text': 'I had a terrible time at the hotel. The staff was rude and the food was awful.'},  \n  {'id': '3', 'language': 'es', 'text': 'Los caminos que llevan hasta Monte Rainier son espectaculares y hermosos.'},  \n  {'id': '4', 'language': 'es', 'text': 'La carretera estaba atascada. Había mucho tráfico el día de ayer.'}\n]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's analyze the text using the Sentiment Analysis API to output a sentiment analysis score:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\nresponse  = requests.post(sentiment_api_url, headers=headers, json=documents)\nsentiments = response.json()\npprint(sentiments)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create another document set with varying degree of sentiment and use the Sentiment Analysis API to detect what\n# the sentiment is",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Key Phrases API"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We've detected the language type using the Text Analytics API and the sentiment using the Sentiment Analysis API. What if we want to detect key phrases in the text? We can use the Key Phrase API."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# As with the other services, setup the Key Phrases API with the following parameters\nkey_phrase_api_url = text_analytics_base_url + \"keyPhrases\"\nprint(key_phrase_api_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create the documents needed to pass to the Key Phrases API with the `id` and `text` attributes."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "documents = {'documents' : [\n  {'id': '1', 'language': 'en', 'text': 'I had a wonderful experience! The rooms were wonderful and the staff was helpful.'},\n  {'id': '2', 'language': 'en', 'text': 'I had a terrible time at the hotel. The staff was rude and the food was awful.'},  \n  {'id': '3', 'language': 'es', 'text': 'Los caminos que llevan hasta Monte Rainier son espectaculares y hermosos.'},  \n  {'id': '4', 'language': 'es', 'text': 'La carretera estaba atascada. Había mucho tráfico el día de ayer.'}\n]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, call the Key Phrases API with the formatted documents to retrieve the key phrases."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "headers   = {'Ocp-Apim-Subscription-Key': subscription_key}\nresponse  = requests.post(key_phrase_api_url, headers=headers, json=documents)\nkey_phrases = response.json()\npprint(key_phrases)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can make this easier to read by outputing the documents in an HTML table format."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "table = []\nfor document in key_phrases[\"documents\"]:\n    text    = next(filter(lambda d: d[\"id\"] == document[\"id\"], documents[\"documents\"]))[\"text\"]    \n    phrases = \",\".join(document[\"keyPhrases\"])\n    table.append(\"<tr><td>{0}</td><td>{1}</td>\".format(text, phrases))\nHTML(\"<table><tr><th>Text</th><th>Key phrases</th></tr>{0}</table>\".format(\"\\n\".join(table)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now call the Key Phrases API with the formatted documents to retrive the key phrases. "
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What other key phrases can you come up with for analysis?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Entities API"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The final API we will use in the Text Analytics API service is the Entities API. This will retrieve attributes for documents provided to the API service."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Configure the Entities URI\nentity_linking_api_url = text_analytics_base_url + \"entities\"\nprint(entity_linking_api_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next step is creating a document with id and text attributes to pass on to the Entities API. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "documents = {'documents' : [\n  {'id': '1', 'text': 'Microsoft is an It company.'}\n]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, call the service using the rest call below to retrieve the data listed in the text attribute."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\nresponse = requests.post(entity_linking_api_url, headers=headers, json=documents)\nentities = response.json()\nentities",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What other entities can be retrieved with the API?\n# Create a document setup and use the Text Analytics, Sentiment Analysis, \n# Key Phrase, and Entities API services to retrieve the data.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** In this subsection, you explored text analytics in the cloud. Specifically, you used a variety of different APIs to extract different information from text: language, sentiment, key phrases, and entities.\n\nThat's it the instructional portion of this course. In these eight sections, you've now seen the range of tools that go into preparing data for analysis and performing ML and AI analysis on data. In the next, concluding section, you will bring these skills together in a final project."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}